import gym
import numpy as np

class AdCampaignEnv(gym.Env):
    def __init__(self):
        super(AdCampaignEnv, self).__init__()
        
        # Load dataset
        self.data = pd.read_csv(data_path)

        # Split into train and test sets
        train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
        self.data = train_data if train else test_data
        
        # Normalize numerical columns
        self.data['budget'] = self.data['budget'] / self.data['budget'].max()
        self.data['impressions'] = self.data['impressions'] / self.data['impressions'].max()
        self.data['clicks'] = self.data['clicks'] / self.data['clicks'].max()
        self.data['conversions'] = self.data['conversions'] / self.data['conversions'].max()

        # 2 Normalize numerical columns
        # self.columns_to_discretize = ['budget', 'impressions', 'clicks', 'conversions', 'cost_per_click']
        # self.bins = bins
        # self.bin_edges = {}
        
        for col in self.columns_to_discretize:
            self.bin_edges[col] = np.linspace(self.data[col].min(), self.data[col].max(), bins)
            self.data[col] = np.digitize(self.data[col], self.bin_edges[col]) - 1    

        # State space: budget, impressions, clicks, conversions
        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)

        # 2 State space: Discretized budget, impressions, clicks, conversions, cost_per_click
        # self.observation_space = gym.spaces.MultiDiscrete([bins] * len(self.columns_to_discretize))
        
        # Action space: budget allocation to each channel
        self.action_space = gym.spaces.Discrete(len(self.data['channel'].unique()))
        
        
        # Action space: budget allocation to each channel
        self.action_space = gym.spaces.Discrete(len(self.data['channel'].unique()))
        
        # Available channels
        self.channels = self.data['channel'].unique()
        
        # Available channels
        # self.channels = ['Google Ads', 'Facebook Ads', 'Instagram Ads', 'Twitter Ads', 'LinkedIn Ads']
        
        # Initial state
        self.current_index = 0
        self.state = self._get_state()
        
    def _get_state(self):
        row = self.data.iloc[self.current_index]
        return np.array([row['budget'], row['impressions'], row['clicks'], row['conversions']], dtype=np.float32)
    
    def step(self, action):
        # Simulating ad campaign performance based on action
        row = self.data.iloc[self.current_index]
        reward = row['roi']  # Using actual ROI as reward
        self.current_index = (self.current_index + 1) % len(self.data)
        self.state = self._get_state()
        done = self.current_index == 0  # End episode when dataset loops
        return self.state, reward, done, {}
    
    def reset(self):
        self.current_index = 0
        self.state = self._get_state()
        return self.state
    
    def render(self, mode='human'):
        print(f"State: {self.state}")

# Environment test
if __name__ == "__main__":
    env = AdCampaignEnv("ad_campaigns.csv")  # Replace with actual CSV path
    state = env.reset()
    for _ in range(10):
        action = env.action_space.sample()
        next_state, reward, done, _ = env.step(action)
        print(f"Action: {action}, Reward: {reward:.2f}, Next State: {next_state}")
        if done:
            break

# Q-learning agent
class QLearningAgent:
    def __init__(self, state_size, action_size, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):
        self.state_size = state_size
        self.action_size = action_size
        self.alpha = alpha  # Learning rate
        self.gamma = gamma  # Discount factor
        self.epsilon = epsilon  # Exploration rate
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.q_table = np.zeros((state_size, action_size))
    
    def choose_action(self, state):
        if np.random.rand() < self.epsilon:
            return random.choice(range(self.action_size))  # Explore
        return np.argmax(self.q_table[state])  # Exploit
    
    def update_q_table(self, state, action, reward, next_state):
        best_next_action = np.argmax(self.q_table[next_state])
        self.q_table[state, action] = (1 - self.alpha) * self.q_table[state, action] + self.alpha * (reward + self.gamma * self.q_table[next_state, best_next_action])
        
        # Decay epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# COMPARE WITH RANDOM AGENT

# Random policy agent
class RandomAgent:
    def __init__(self, action_size):
        self.action_size = action_size
    
    def choose_action(self, _state):
        return random.choice(range(self.action_size))

# Train Q-learning agent
if __name__ == "__main__":
    env = AdCampaignEnv("ad_campaigns.csv", train=True)  # Train environment
    agent = QLearningAgent(state_shape=env.observation_space.nvec, action_size=len(env.channels))
    
    episodes = 1000
    rewards_history = []
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False
        while not done:
            action = agent.choose_action(state)
            next_state, reward, done, _ = env.step(action)
            agent.update_q_table(state, action, reward, next_state)
            state = next_state
            total_reward += reward
        rewards_history.append(total_reward)
        print(f"Episode {episode + 1}: Total Reward = {total_reward}")
    
    # Evaluate Q-learning agent
    test_env = AdCampaignEnv("ad_campaigns.csv", train=False)
    test_rewards = []
    for _ in range(100):  # Run 100 test episodes
        state = test_env.reset()
        total_reward = 0
        done = False
        while not done:
            action = agent.choose_action(state)
            state, reward, done, _ = test_env.step(action)
            total_reward += reward
        test_rewards.append(total_reward)
    
    print(f"Q-learning Average Test Reward: {np.mean(test_rewards)}")
    
    # Evaluate Random Agent
    random_agent = RandomAgent(action_size=len(env.channels))
    random_rewards = []
    for _ in range(100):  # Run 100 test episodes
        state = test_env.reset()
        total_reward = 0
        done = False
        while not done:
            action = random_agent.choose_action(state)
            state, reward, done, _ = test_env.step(action)
            total_reward += reward
        random_rewards.append(total_reward)
    
    print(f"Random Policy Average Test Reward: {np.mean(random_rewards)}")

# END OF RANDOM AGENT COMPARISON

# Train agent
if __name__ == "__main__":
    env = AdCampaignEnv("ad_campaigns.csv", train=True)  # Train environment
    agent = QLearningAgent(state_shape=env.observation_space.nvec, action_size=len(env.channels))
    
    episodes = 1000
    rewards_history = []
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False
        while not done:
            action = agent.choose_action(state)
            next_state, reward, done, _ = env.step(action)
            agent.update_q_table(state, action, reward, next_state)
            state = next_state
            total_reward += reward
        rewards_history.append(total_reward)
        print(f"Episode {episode + 1}: Total Reward = {total_reward}")

# Evaluate on test set
    test_env = AdCampaignEnv("ad_campaigns.csv", train=False)
    test_rewards = []
    for _ in range(100):  # Run 100 test episodes
        state = test_env.reset()
        total_reward = 0
        done = False
        while not done:
            action = agent.choose_action(state)
            state, reward, done, _ = test_env.step(action)
            total_reward += reward
        test_rewards.append(total_reward)
    
    print(f"Average Test Reward: {np.mean(test_rewards)}")

# Evaluate on unseen test data
  def evaluate_on_test_data(agent, env):
      state = env.reset()
      total_reward = 0
      done = False
      while not done:
          action = agent.choose_action(state)
          next_state, reward, done, _ = env.step(action)
          state = next_state
          total_reward += reward
      return total_reward
  
  env_test = AdCampaignEnv("ad_campaigns.csv", train=False)
  agent_test = QLearningAgent(state_shape=env_test.observation_space.nvec, action_size=len(env_test.channels), alpha=best_params[0], gamma=best_params[1], epsilon=0.01, epsilon_decay=best_params[2])
  
  test_reward = evaluate_on_test_data(agent_test, env_test)
  heuristic_reward = evaluate_on_test_data(agent_test, env_test)  # Reuse function for heuristic
  
  print(f"Q-learning Test Reward: {test_reward}")
  print(f"Heuristic Baseline Reward: {heuristic_reward}")


  # Heuristic allocation: Budget proportional to past ROI
  def heuristic_budget_allocation_roi(data):
      roi_per_channel = data.groupby('channel')['roi'].sum()
      total_roi = roi_per_channel.sum()
      budget_allocation_roi = roi_per_channel / total_roi
      return budget_allocation_roi

  # Heuristic allocation: Budget proportional to past conversions
  def heuristic_budget_allocation_conversions(data):
    conversions_per_channel = data.groupby('channel')['conversions'].sum()
    total_conversions = conversions_per_channel.sum()
    budget_allocation_conversions = conversions_per_channel / total_conversions
    return budget_allocation_conversions

  # Evenly split budget across all channels
  def even_budget_allocation(data):
      num_channels = len(data['channel'].unique())
      budget_allocation_evenly = {channel: 1 / num_channels for channel in data['channel'].unique()}
      return budget_allocation_evenly

  # Hyperparameter tuning grid search
  alpha_values = [0.1, 0.2, 0.5]
  gamma_values = [0.8, 0.9, 0.99]
  epsilon_decay_values = [0.995, 0.99, 0.98]
  
  best_params = None
  best_reward = -np.inf
  
  for alpha, gamma, epsilon_decay in product(alpha_values, gamma_values, epsilon_decay_values):
      env = AdCampaignEnv("ad_campaigns.csv", train=True)  # Train environment
      agent = QLearningAgent(state_shape=env.observation_space.nvec, action_size=len(env.channels), alpha=alpha, gamma=gamma, epsilon=1.0, epsilon_decay=epsilon_decay)
      
      episodes = 500
      total_rewards = []
      
      for episode in range(episodes):
          state = env.reset()
          total_reward = 0
          done = False
          while not done:
              action = agent.choose_action(state)
              next_state, reward, done, _ = env.step(action)
              agent.update_q_table(state, action, reward, next_state)
              state = next_state
              total_reward += reward
          total_rewards.append(total_reward)
      
      avg_reward = np.mean(total_rewards)
      print(f"Alpha: {alpha}, Gamma: {gamma}, Epsilon Decay: {epsilon_decay} => Avg Reward: {avg_reward}")
      
      if avg_reward > best_reward:
          best_reward = avg_reward
          best_params = (alpha, gamma, epsilon_decay)
  
  print(f"Best Hyperparameters: Alpha={best_params[0]}, Gamma={best_params[1]}, Epsilon Decay={best_params[2]}")
  
  # Heuristic allocation output
  env_test = AdCampaignEnv("ad_campaigns.csv", train=False)
  heuristic_allocation_roi = heuristic_budget_allocation_roi(env_test.data)
  print("Heuristic Budget Allocation ROI:")
  print(heuristic_allocation_roi)

  # Heuristic allocation output
  env_test = AdCampaignEnv("ad_campaigns.csv", train=False)
  heuristic_allocation_conversions = heuristic_budget_allocation_conversions(env_test.data)
  print("Heuristic Budget Allocation Past Conversions:")
  print(heuristic_allocation_conversions)

  # Even budget allocation output
  even_allocation = even_budget_allocation(env_test.data)
  print("Even Budget Allocation:")
  print(even_allocation)

# Plot reward progression
    plt.figure(figsize=(10, 5))
    plt.plot(rewards_history, label='Total Reward per Episode')
    plt.xlabel('Episodes')
    plt.ylabel('Total Reward')
    plt.title('Q-learning Training Progress')
    plt.legend()
    plt.show()
    
    # Plot Q-table heatmap
    plt.figure(figsize=(8, 6))
    sns.heatmap(agent.q_table.mean(axis=-1), cmap='coolwarm', annot=False)
    plt.xlabel("Actions (Channels)")
    plt.ylabel("States")
    plt.title("Q-Table Heatmap")
    plt.show()

# Compute average ROI per channel
def compute_avg_roi(data):
    avg_roi = data.groupby('channel')['roi'].mean()
    print("Average ROI per Channel:")
    print(avg_roi)
    return avg_roi

env_test = AdCampaignEnv("ad_campaigns.csv", train=False)
compute_avg_roi(env_test.data)

# Visualization of ROI trends
def visualize_roi_trends(data):
    plt.figure(figsize=(10, 5))
    sns.lineplot(data=data, x='budget', y='roi', hue='channel', marker='o')
    plt.xlabel("Budget")
    plt.ylabel("ROI")
    plt.title("ROI Trends per Channel")
    plt.legend(title="Channel")
    plt.show()

visualize_roi_trends(env_test.data)
